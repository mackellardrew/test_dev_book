{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In which the author attempts to justify his aesthetic vision of code and tests thereof\n",
    "\n",
    "Ok, at this point, Harry is heading off criticism and concerns about the plodding pace of the unit tests introduced in Chapter 3.  He implies that if you have such feedback, you're far too naive about writing maintainable code.  He suggests building discipline in forcing yourself to follow the model of making *really* incremental changes whenever you apply TDD; the idea is to minimize the amount of work you have to re-do whenever your code fails.  If you break it up into small enough pieces, and test each separately, addressing each (inevitable) failure becomes a lot simpler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, then we resume adding stuff to `functional_tests.py` to address the issue encountered at the end of the last chapter where there was just a placeholder print statement reminding us that we hadn't done that yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting functional_tests.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile functional_tests.py\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import unittest\n",
    "\n",
    "class NewVisitorTest(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        self.browser = webdriver.Firefox()\n",
    "\n",
    "    def tearDown(self):\n",
    "        self.browser.quit()\n",
    "\n",
    "    def test_can_start_a_list_and_retrieve_it_later(self):\n",
    "        # Edith has heard about a cool new online to-do app.  She goes\n",
    "        # to check out its homepage\n",
    "        self.browser.get('http://localhost:8000')\n",
    "\n",
    "        # She notices the page title and header mention to-do lists\n",
    "        self.assertIn('To-Do', self.browser.title)\n",
    "        header_text = self.browser.find_element_by_tag_name('h1').text\n",
    "        self.assertIn('To-Do', header_text)\n",
    "\n",
    "        # She is invited to enter a to-do item straight away\n",
    "        inputbox = self.browser.find_element_by_id('id_new_item')\n",
    "        self.assertEqual(\n",
    "            inputbox.get_attribute('placeholder'),\n",
    "            'Enter a to-do item'\n",
    "        )\n",
    "\n",
    "        # She types \"Buy peacock feathers\" into a text box \n",
    "        # (Edith's hobby is tying fly-fishing lures)\n",
    "        inputbox.send_keys('Buy peacock feathers')\n",
    "\n",
    "        # When she hits enter, the page updates, and now the page lists\n",
    "        # \"1: Buy peacock feathers\" as an item in a to-do list table\n",
    "        inputbox.send_keys(Keys.ENTER)\n",
    "        time.sleep(1)\n",
    "\n",
    "        table = self.browser.find_element_by_id('id_list_table')\n",
    "        rows = table.find_elements_by_tag_name('tr')\n",
    "        self.assertTrue(\n",
    "            any(row.text == '1: Buy peacock feathers' for row in rows)\n",
    "        )\n",
    "\n",
    "        # There is still a text box inviting her to add another item.\n",
    "        # Shee enters \"Use peacock feathers to make a fly\"\n",
    "        # (Edith is very methodical)\n",
    "        self.fail('Finish the test!')\n",
    "\n",
    "        # The page updates again, and now shows both items on her list\n",
    "        # ...\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(warnings='ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I'm just a humble passive-aggressive misanthrope with a bum knee and an array of interesting sub-clinical manifestations of various other personality disorders, but to me that looks a lot *less* incremental than was preached so far.  So we'll see how he plays it.\n",
    "\n",
    "First, we summarize the main effects of the above:\n",
    "\n",
    "1. It introduces a few major functions from Selenium's `selenium.browser` object: `find_element_by_tag_name`, and so forth.\n",
    "2. We point out that the `selenium.browser.send_keys` is the main way to simulate user input to Selenium\n",
    "3. To use `send_keys`, though, we also have to import `selenium.webdriver.common.keys.Keys`, to access all the major QWERTY keyboard keys that you may need\n",
    "4. The page should refresh after the `send_keys(Keys.ENTER)` command (line 38), so that `time.sleep` call is meant to give the server time to catch up before testing the assertions below; apparently this is called an \"*explicit wait*\"\n",
    "\n",
    "Then, fire up the dev server and run the updated functional tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E\n",
      "======================================================================\n",
      "ERROR: test_can_start_a_list_and_retrieve_it_later (__main__.NewVisitorTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Data\\projects\\test_driven_development\\functional_tests.py\", line 21, in test_can_start_a_list_and_retrieve_it_later\n",
      "    header_text = self.browser.find_element_by_tag_name('h1').text\n",
      "  File \"C:\\Users\\DCM0303\\Miniconda3\\envs\\test_dev_book\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\", line 530, in find_element_by_tag_name\n",
      "    return self.find_element(by=By.TAG_NAME, value=name)\n",
      "  File \"C:\\Users\\DCM0303\\Miniconda3\\envs\\test_dev_book\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\", line 978, in find_element\n",
      "    'value': value})['value']\n",
      "  File \"C:\\Users\\DCM0303\\Miniconda3\\envs\\test_dev_book\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\", line 321, in execute\n",
      "    self.error_handler.check_response(response)\n",
      "  File \"C:\\Users\\DCM0303\\Miniconda3\\envs\\test_dev_book\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\", line 242, in check_response\n",
      "    raise exception_class(message, screen, stacktrace)\n",
      "selenium.common.exceptions.NoSuchElementException: Message: Unable to locate element: h1\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 6.775s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "True",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m True\n"
     ]
    }
   ],
   "source": [
    "%run functional_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so it can't find an `h1` tag in the loaded HTML.  But at this point, he recommends another commit, to store the changes to the `functional_tests.py` file.\n",
    "\n",
    "\n",
    "# The \"Don't Test Constants\" Rule, and Templates to the Rescue\n",
    "\n",
    "He notes that the unit tests currently explicitly test for HTML tags as explicit text, which isn't how you'd do any of this if you're an actual web dev; that was just a basic way to show off that functionality before making the example a little more complex.  One TDD mantra is to skip testing constants; you wouldn't want to write `assert thing == blah` to test as trivial a line in your production code as \"`thing = blah`\".  And apparently that level of granularity applies analogously to HTML tags:\n",
    "\n",
    "\"*Unit tests are really about testing logic, flow control, and configuration.  Making assertions about exactly what sequence of characters we have in our HTML strings isn't doing that.*\"\n",
    "\n",
    "Furthermore, web devs don't really format HTML by writing raw tags into strings themselves; there are plenty of templates and frameworks that can do a lot of that boilerplate work for you.  Obviously, some are built into Django, so he suggests refactoring our pre-existing calls to add tags as text to instead rely on some templates.\n",
    "\n",
    "\n",
    "## Refactoring to Use a Template\n",
    "\n",
    "We define *refactoring* as changing code to achieve the same functionality as before, and specify that you should execute it on its own, rather than simultaneously refactoring *and* adding more functionality within a single commit.  Finally, you want to set up tests to try to be *sure* about the whole \"same functionality\" part.\n",
    "\n",
    "We start by making a file to hold our templates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lists/templates/home.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile lists/templates/home.html\n",
    "<html>\n",
    "    <title>To-Do lists</title>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, apparently, we also need to get rid of the explicit references to `HttpResponse` objects in our `views` file, and instead call the `django.shortcuts.render` function.  This will have Django look for folders named like \"`templates`\", and return the specific HTML file you request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lists/views.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lists/views.py\n",
    "from django.shortcuts import render\n",
    "\n",
    "def home_page(request):\n",
    "    return render(request, 'home.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test database for alias 'default'..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.019s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "System check identified no issues (0 silenced).\n",
      "Destroying test database for alias 'default'...\n"
     ]
    }
   ],
   "source": [
    "# %run manage test\n",
    "!python manage.py test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divergent Results\n",
    "\n",
    "Ok, at this point, my test is returning \"`OK`\", like everything's passing, but in the text Harry says we should see instead an\n",
    "\n",
    "`ERROR: test_home_page_returns_correct_html (lists.tests.HomePageTest)`\n",
    "\n",
    "where the traceback says at the bottom:\n",
    "\n",
    "`django.template.base.TemplateDoesNotExist: home.html`\n",
    "\n",
    "Which then prompts us to explicitly register our `lists` subdir with the `superlists/settings.py` file; specifically, in the `INSTALLED_APPS` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "django.contrib.admin\n",
      "django.contrib.auth\n",
      "django.contrib.contenttypes\n",
      "django.contrib.sessions\n",
      "django.contrib.messages\n",
      "django.contrib.staticfiles\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "from superlists import settings\n",
    "\n",
    "for name, element in inspect.getmembers(settings):\n",
    "    if name == 'INSTALLED_APPS':\n",
    "        print('\\n'.join(element))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Harry says that we should have to manually add `'lists'` to that list.  Yet my test says it's passing right now, as-is.  Maybe it's a versioning difference with Django or something?\n",
    "\n",
    "...Well, I just double-checked, and my conda env for this book is utilizing Django version `1.11.25`, and the Prereqs portion of the book just specifies we should be using Django 1.11, so I don't know if that's really a credible explanation for the difference.  Just move on, and see if other changes emerge that would allow me to troubleshoot better...\n",
    "\n",
    "In the text, once Harry registers `lists` with `INSTALLED_APPS` in `superlists/settings.py`, he reports another error about the assertion that the home page `endswith('</html>')`, but I don't see that either.  In this case, though, he notes that you might not *always* see the error; apparently it has to do with a missing newline inserted at the end of the template.  My approach didn't add that, so I guess it's still \"`OK`\".\n",
    "\n",
    "...Note: subsequently, after making the changes below, I *did* start to see an error that led me to manually add `lists` to `superlists/settings.py`.  Once I had done so, I *did* also hit the `AssertionError` that caused me to have to add the `strip()` call to the `lists/tests.py` file's `test_home_page_returns_correct_html` func.  Weird; I have no idea what caused the initial discrepancy, nor the subsequent change to become concordant with Harry's declared outputs...\n",
    "\n",
    "Anyways, that simple replacement of the `HttpResponse(\"<text>\")` element with the `render(template_file)` kind of approach constitutes the entirety of our \"refactor\" at this point.  So we're moving back to focusing on testing.  But Harry emphasizes that, in keeping with the spirit of refactor of the raw HTML input with a template file instead, we want to change what we're testing to look not for specific tags but more general ways to ensure that a response is returning the right template file.\n",
    "\n",
    "\n",
    "# The Django Test Client\n",
    "\n",
    "Harry says that the [Django Test Client](https://docs.djangoproject.com/en/1.11/topics/testing/tools/#the-test-client) is meant to intelligently detect which templates are used to generate a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lists/tests.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lists/tests.py\n",
    "from django.urls import resolve\n",
    "from django.test import TestCase\n",
    "from django.http import HttpRequest\n",
    "\n",
    "from lists.views import home_page\n",
    "\n",
    "class HomePageTest(TestCase):\n",
    "\n",
    "    def test_root_url_resolves_to_home_page_view(self):\n",
    "        found = resolve('/')\n",
    "        self.assertEqual(found.func, home_page)\n",
    "\n",
    "    def test_home_page_returns_correct_html(self):\n",
    "        response = self.client.get('/')\n",
    "\n",
    "        html = response.content.decode('utf8')\n",
    "        self.assertTrue(html.startswith('<html>'))\n",
    "        self.assertIn('<title>To-Do lists</title>', html)\n",
    "        self.assertTrue(html.endswith('</html>'))\n",
    "\n",
    "        self.assertTemplateUsed(response, 'home.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SO the only changes above are in the `test_home_page_returns_correct_html` func, with the removal of lines saying to generate the response via\n",
    "\n",
    "~~~python\n",
    "request = HttpRequest()\n",
    "response = home_page(request)\n",
    "~~~\n",
    "\n",
    "and instead to replace them with that single call to\n",
    "\n",
    "~~~python\n",
    "response = self.client.get('/')\n",
    "~~~\n",
    "\n",
    "and finally, the addition of the final line in that func:\n",
    "\n",
    "~~~python\n",
    "self.assertTemplateUsed(response, 'home.html')\n",
    "~~~\n",
    "\n",
    "Which sounds like the more intelligent comparison method mentioned above for ensuring that the view returned comes from a certain template file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test database for alias 'default'...\n",
      "System check identified no issues (0 silenced).\n",
      "Destroying test database for alias 'default'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.020s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python manage.py test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Harry then says that if you have any doubts about the effectiveness of a new test, you should purposely cause it to fail; in this case, just substitute \"`wrong.html`\" for \"`home.html`\" in the final line of the `lists/tests.py` file, and it'll tell you that a different template was used.\n",
    "\n",
    "This new change allows you to remove a lot of the other assertions in the function we've just modified, as well as to get rid of the original unit test func:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lists/tests.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lists/tests.py\n",
    "from django.test import TestCase\n",
    "\n",
    "class HomePageTest(TestCase):\n",
    "\n",
    "    def test_home_page_returns_correct_html(self):\n",
    "        response = self.client.get('/')\n",
    "        self.assertTemplateUsed(response, 'home.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And ensure that it gives the same result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test database for alias 'default'...\n",
      "System check identified no issues (0 silenced).\n",
      "Destroying test database for alias 'default'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.023s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python manage.py test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we commit again:\n",
    "\n",
    "~~~bash\n",
    "git status\n",
    "git add lists/templates/.\n",
    "git diff --staged\n",
    "git commit -m \"Refactor home page view to use a template\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding to the Front Page\n",
    "\n",
    "He says the functional test should still be failing (it's yielding the `NoSuchElementException`, unable to locate an `h1` tag).  Harry says that at this point it's fine to add something within an `h1` (header) tag within a `body` tag to our template, without necessarily needing to write new unit tests to detect those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lists/templates/home.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile lists/templates/home.html\n",
    "<html>\n",
    "    <head>\n",
    "        <title>To-Do lists</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>Your To-Do list</h1>\n",
    "    </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python functional_tests.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, at this point, he has us make multiple incremental changes to the template file, then re-run the functional test file.  Each time, however, it outputs a full traceback and a new error, because we've overlooked adding other elements that the test is looking for.  You can check them in the `functional_tests.py` file itself, but at this point, rather than cluttering up the notebook with multiple such iterations of the functional tests' output, I'll just make all of the modifications to the template and check the result.  We need to add:\n",
    "\n",
    "1. Text within `h1` tags (done)\n",
    "2. A placeholder with an `id` of \"`id_new_item`\"\n",
    "3. A placeholder with `id` of \"`id_list_table`\"\n",
    "\n",
    "To the template, but then he also notes that there's an issue with the functional test itself; specifically (in my current text), in line 43, the line checking for the text of \"`1: Buy peacock feathers`\" within `any` row of the `id_list_table`.  Specifically, since that table is just a placeholder at this point, this test fails and it returns a not-very-informative failure message of \"`False is not true`\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lists/templates/home.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile lists/templates/home.html\n",
    "<html>\n",
    "    <head>\n",
    "        <title>To-Do lists</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>Your To-Do list</h1>\n",
    "        <input id=\"id_new_item\" placeholder=\"Enter a to-do item\" />\n",
    "        <table id=\"id_list_table\">\n",
    "        </table>\n",
    "    </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting functional_tests.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile functional_tests.py\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import unittest\n",
    "\n",
    "class NewVisitorTest(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        self.browser = webdriver.Firefox()\n",
    "\n",
    "    def tearDown(self):\n",
    "        self.browser.quit()\n",
    "\n",
    "    def test_can_start_a_list_and_retrieve_it_later(self):\n",
    "        # Edith has heard about a cool new online to-do app.  She goes\n",
    "        # to check out its homepage\n",
    "        self.browser.get('http://localhost:8000')\n",
    "\n",
    "        # She notices the page title and header mention to-do lists\n",
    "        self.assertIn('To-Do', self.browser.title)\n",
    "        header_text = self.browser.find_element_by_tag_name('h1').text\n",
    "        self.assertIn('To-Do', header_text)\n",
    "\n",
    "        # She is invited to enter a to-do item straight away\n",
    "        inputbox = self.browser.find_element_by_id('id_new_item')\n",
    "        self.assertEqual(\n",
    "            inputbox.get_attribute('placeholder'),\n",
    "            'Enter a to-do item'\n",
    "        )\n",
    "\n",
    "        # She types \"Buy peacock feathers\" into a text box \n",
    "        # (Edith's hobby is tying fly-fishing lures)\n",
    "        inputbox.send_keys('Buy peacock feathers')\n",
    "\n",
    "        # When she hits enter, the page updates, and now the page lists\n",
    "        # \"1: Buy peacock feathers\" as an item in a to-do list table\n",
    "        inputbox.send_keys(Keys.ENTER)\n",
    "        time.sleep(1)\n",
    "\n",
    "        table = self.browser.find_element_by_id('id_list_table')\n",
    "        rows = table.find_elements_by_tag_name('tr')\n",
    "        self.assertTrue(\n",
    "            any(row.text == '1: Buy peacock feathers' for row in rows),\n",
    "            \"New to-do item did not appear in table\"\n",
    "        )\n",
    "\n",
    "        # There is still a text box inviting her to add another item.\n",
    "        # Shee enters \"Use peacock feathers to make a fly\"\n",
    "        # (Edith is very methodical)\n",
    "        self.fail('Finish the test!')\n",
    "\n",
    "        # The page updates again, and now shows both items on her list\n",
    "        # ...\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(warnings='ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, repeat the functional test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F\n",
      "======================================================================\n",
      "FAIL: test_can_start_a_list_and_retrieve_it_later (__main__.NewVisitorTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"functional_tests.py\", line 44, in test_can_start_a_list_and_retrieve_it_later\n",
      "    \"New to-do item did not appear in table\"\n",
      "AssertionError: False is not true : New to-do item did not appear in table\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 7.739s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    }
   ],
   "source": [
    "!python functional_tests.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we've gotten as far as executing line #44 (#45 in the above cell because we have the first line to write the cell to file, which doesn't appear in the resulting file), and are matched up with the text.\n",
    "\n",
    "There's a flowchart diagram (that I still need to learn how to recreate in matplotlib) summarizing the overall idea in TDD:\n",
    "\n",
    "1. Write a test\n",
    "2. Run the test\n",
    "    * If it fails (expected since you haven't written prod code yet), go to #3\n",
    "    * If it passes, go to #4\n",
    "3. Write minimal dev/prod code\n",
    "    * Go to #2\n",
    "4. Does it need refactoring?\n",
    "    * If yes, go to #3\n",
    "    * If no, go to #1\n",
    "\n",
    "Clearly, it's more elegant as a schematic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But a good point that's brought up is that functional tests (again, as attempts to capture the \"story\" of how an end user might interact with your app) are more complex, so you can approach them a bit like apps of their own, and still apply the above cycle.  Basically, start with the functional test, and for each chunk of it that you want to get past, add new unit tests and dev/prod/app code to accomplish that, then move down to the next function in your functional test.\n",
    "\n",
    "Commit this chapter to the repo, and move on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "132b65336a829e6530b03c2ac43b744d0aab8a1a430c616a24f2cd56f79cc52e"
  },
  "kernelspec": {
   "display_name": "Python 3.6.15 64-bit ('test_dev_book': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
