{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tidying Up\n",
    "\n",
    "In the course of presenting certain common problems and short-term solutions in the last chapter, we've introduced some kind of janky code, and this chapter starts with trying to replace them with alternatives that represent better practices.\n",
    "\n",
    "\n",
    "## Isolation\n",
    "\n",
    "He speaks first of ensuring that functional tests are *isolated* from one another, but I don't think we're talking (necessarily) about the different functions in the `functional_tests.py` file, but rather the idea of isolating the different instances/calls to the file, so that, as we saw at the end of Chapter 5, we don't have different entries specified by the test piling up in our database file.\n",
    "\n",
    "For that, the preferred solution is to specify either a separate database from the \"production\" one be modified by functional tests, or else that any changes logged to it are automatically undone once testing is complete.  Harry says that adding logic to your `setUp` and `tearDown` methods in your functional tests file (again, which extends `unittest.TestCase`), but the cleaner approach is to utilize the `LiveServerTestCase` object that's built into Django since version 1.4.\n",
    "\n",
    "At this point, he suggests mvoing the functional tests file into a different subdir named the same thing.  You do that via `git`, rather than just the OS alone, to ensure that your repo's tracking of the move is logged and handled appropriately.  You also need to add an (empty) `init` file to the new subdir, to have the Python interpreter treat it like its own module.  Then, Django should parse the files therein and execute any that start with `test` whenever your execute a functional test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "Path('functional_tests').mkdir(exist_ok=True)\n",
    "with open('functional_tests/__init__.py', 'a') as f:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment out after executing once:\n",
    "# !git mv functional_tests.py functional_tests/tests.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the new syntax to call for functional tests is \n",
    "\n",
    "`python manage.py test functional_tests`\n",
    "\n",
    "i.e., point to it as a module.\n",
    "\n",
    "<div class=\"alert alert-info\"><p>There's a box here in the text noting that you <i>could</i> throw your functional test files into that same <code>lists</code> subdir containing the unit tests, but that a lot of times functional tests (which, again, are attempting to simulate what a user might go through when visiting your site) will need to interact with multiple apps in a site, an therefore make sense to organize separately.</p>\n",
    "<p>I would also note that, at this point, I guess I've been thinking that the \"<code>lists</code>\" subdir somehow reflected something about how Django tends to organize things, i.e., that \"lists\" were somehow something universal to web sites in general.  But now that I'm prompted to consider the organization of our project dir anew, I guess I see that it's meant instead to constitute a distinct \"app\", meaning it simply acknowledges that the point of the main function of our site so far is to store \"To-Do\" lists.  In general, as I get more experience, it should become more obvious what aspects of the organization of such projects are specific to the way Django expects things to be structured, versus things that we're free to set up however we like, and how to modify our settings and commands accordingly, to have the framework make sense of our projects as we intend.</p></div>\n",
    "\n",
    "Now, we just modify the renamed functional tests file to utilize the aforementioned `LiveServerTestCase` class.  The other changes made at this point to the file are to remove the reference to the explicit URL and port number in the `retrieve_it_later` method, since this new object has a dedicated `live_server_url` attr that stores such things, and also that we can delete the \"`if name`\" block at the bottom that renders the file executable as a separate Python script, since we intend to call it in the future via tha `manage.py` command, which will interpret this as a valid set of tests without direct handling from the interpreter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting functional_tests/tests.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile functional_tests/tests.py\n",
    "from django.test import LiveServerTestCase\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import unittest\n",
    "\n",
    "class NewVisitorTest(LiveServerTestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        self.browser = webdriver.Firefox()\n",
    "\n",
    "    def tearDown(self):\n",
    "        self.browser.quit()\n",
    "\n",
    "    def check_for_row_in_list_table(self, row_text):\n",
    "        table = self.browser.find_element_by_id('id_list_table')\n",
    "        rows = table.find_elements_by_tag_name('tr')\n",
    "        self.assertIn(row_text, [row.text for row in rows])\n",
    "\n",
    "    def test_can_start_a_list_and_retrieve_it_later(self):\n",
    "        # Edith has heard about a cool new online to-do app.  She goes\n",
    "        # to check out its homepage\n",
    "        self.browser.get(self.live_server_url)\n",
    "\n",
    "        # She notices the page title and header mention to-do lists\n",
    "        self.assertIn('To-Do', self.browser.title)\n",
    "        header_text = self.browser.find_element_by_tag_name('h1').text\n",
    "        self.assertIn('To-Do', header_text)\n",
    "\n",
    "        # She is invited to enter a to-do item straight away\n",
    "        inputbox = self.browser.find_element_by_id('id_new_item')\n",
    "        self.assertEqual(\n",
    "            inputbox.get_attribute('placeholder'),\n",
    "            'Enter a to-do item'\n",
    "        )\n",
    "\n",
    "        # She types \"Buy peacock feathers\" into a text box \n",
    "        # (Edith's hobby is tying fly-fishing lures)\n",
    "        inputbox.send_keys('Buy peacock feathers')\n",
    "\n",
    "        # When she hits enter, the page updates, and now the page lists\n",
    "        # \"1: Buy peacock feathers\" as an item in a to-do list table\n",
    "        inputbox.send_keys(Keys.ENTER)\n",
    "        time.sleep(1)\n",
    "        self.check_for_row_in_list_table('1: Buy peacock feathers')\n",
    "\n",
    "        # There is still a text box inviting her to add another item.\n",
    "        # She enters \"Use peacock feathers to make a fly\"\n",
    "        # (Edith is very methodical)\n",
    "        inputbox = self.browser.find_element_by_id('id_new_item')\n",
    "        inputbox.send_keys('Use peacock feathers to make a fly')\n",
    "        inputbox.send_keys(Keys.ENTER)\n",
    "        time.sleep(1)\n",
    "\n",
    "        # The page updates again, and now shows both items on her list\n",
    "        self.check_for_row_in_list_table('1: Buy peacock feathers')\n",
    "        self.check_for_row_in_list_table('2: Use peacock feathers to make a fly')\n",
    "\n",
    "        # Edith wonders whether the site will remember her list.  Then she sees\n",
    "        # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test database for alias 'default'...\n",
      "System check identified no issues (0 silenced).\n",
      "Destroying test database for alias 'default'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 11.970s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python manage.py test functional_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point in the text there was still a reference to a line like `self.fail('Finish the test!')`, but I don't have that in mine, so it doesn't return any failures at all.  Anyways, add the new subdir to the project with Git, and commit to the remote.\n",
    "\n",
    "\n",
    "# Handling Waiting is Tricky\n",
    "\n",
    "Ok, at this point in the text, Harry points out that another kind of wonky workaround applied earlier was the \"explicit wait\" strategy embodied by the calls to `time.sleep` in the functional tests file.  There is often some kind of load time involved when testing your server (presumably even longer ones when you're actually testing a remote server over some connection like you'd have in a more realistic web app scenario), and the amount of time to wait is tough to anticipate efficiently.  The risk is you underestimate the load time required, in which case you'll prob see `NoSuchElementException`s and `StaleElementException`s; make your tests wait too long, however, and you'll just be reducing the rate at which you can confirm and fix errors, without yielding improved info about what's broken.\n",
    "\n",
    "He says that Selenium used to try to figure that out for you, employing a strategy called an \"implicit wait\", but that it got too difficult to implement and unpredictable in its behavior, so they deprecated it as a practice.  Instead, there's this kind of hybrid solution, where Selenium *tries* to figure out how long to wait, but you can set a `MAX_WAIT` constant value that will direct its behavior a little more explicitly.  We then change the name of the `check_for_row_in_list_table` method to `wait_for_row_in_list_table`, make it reference that `MAX_WAIT` constant, and then update the later calls within the huge `test_can_start_a_list...` method to point to the renamed method, and remove the explicit `time.sleep` calls after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting functional_tests/tests.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile functional_tests/tests.py\n",
    "from django.test import LiveServerTestCase\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "import time\n",
    "import unittest\n",
    "\n",
    "MAX_WAIT = 10\n",
    "\n",
    "class NewVisitorTest(LiveServerTestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        self.browser = webdriver.Firefox()\n",
    "\n",
    "    def tearDown(self):\n",
    "        self.browser.quit()\n",
    "\n",
    "    def wait_for_row_in_list_table(self, row_text):\n",
    "        start_time = time.time()\n",
    "        while True:\n",
    "            try:\n",
    "                table = self.browser.find_element_by_id('id_list_table')\n",
    "                rows = table.find_elements_by_tag_name('tr')\n",
    "                self.assertIn(row_text, [row.text for row in rows])\n",
    "                return\n",
    "            except (AssertionError, WebDriverException) as e:\n",
    "                if time.time() - start_time > MAX_WAIT:\n",
    "                    raise e\n",
    "                time.sleep(0.5)\n",
    "\n",
    "    def test_can_start_a_list_and_retrieve_it_later(self):\n",
    "        # Edith has heard about a cool new online to-do app.  She goes\n",
    "        # to check out its homepage\n",
    "        self.browser.get(self.live_server_url)\n",
    "\n",
    "        # She notices the page title and header mention to-do lists\n",
    "        self.assertIn('To-Do', self.browser.title)\n",
    "        header_text = self.browser.find_element_by_tag_name('h1').text\n",
    "        self.assertIn('To-Do', header_text)\n",
    "\n",
    "        # She is invited to enter a to-do item straight away\n",
    "        inputbox = self.browser.find_element_by_id('id_new_item')\n",
    "        self.assertEqual(\n",
    "            inputbox.get_attribute('placeholder'),\n",
    "            'Enter a to-do item'\n",
    "        )\n",
    "\n",
    "        # She types \"Buy peacock feathers\" into a text box \n",
    "        # (Edith's hobby is tying fly-fishing lures)\n",
    "        inputbox.send_keys('Buy peacock feathers')\n",
    "\n",
    "        # When she hits enter, the page updates, and now the page lists\n",
    "        # \"1: Buy peacock feathers\" as an item in a to-do list table\n",
    "        inputbox.send_keys(Keys.ENTER)\n",
    "        self.wait_for_row_in_list_table('1: Buy peacock feathers')\n",
    "\n",
    "        # There is still a text box inviting her to add another item.\n",
    "        # She enters \"Use peacock feathers to make a fly\"\n",
    "        # (Edith is very methodical)\n",
    "        inputbox = self.browser.find_element_by_id('id_new_item')\n",
    "        inputbox.send_keys('Use peacock feathers to make a fly')\n",
    "        inputbox.send_keys(Keys.ENTER)\n",
    "\n",
    "        # The page updates again, and now shows both items on her list\n",
    "        self.wait_for_row_in_list_table('1: Buy peacock feathers')\n",
    "        self.wait_for_row_in_list_table('2: Use peacock feathers to make a fly')\n",
    "\n",
    "        # Edith wonders whether the site will remember her list.  Then she sees\n",
    "        # ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test database for alias 'default'...\n",
      "System check identified no issues (0 silenced).\n",
      "Destroying test database for alias 'default'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".......\n",
      "----------------------------------------------------------------------\n",
      "Ran 7 tests in 11.506s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python manage.py test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we've had enough successful tests in a row that the author gets a little nervous, and suggests purposely breaking something, to check that it returns the right response.  One such change involves looking for nonsensical text (\"`'foo'`\") in the functional test's check through the contents of the rows, and another that looks for a nonsensical tag (\"`'id_nothing'`\") in the browser's content.  I'll skip those here, since it takes up so much space to modify these test files in such a way, just to add a single line.\n",
    "\n",
    "Oh, ok; that's all for this chapter.  It was a pretty short one.  Commit and move on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "132b65336a829e6530b03c2ac43b744d0aab8a1a430c616a24f2cd56f79cc52e"
  },
  "kernelspec": {
   "display_name": "Python 3.6.15 64-bit ('test_dev_book': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
