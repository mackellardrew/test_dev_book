{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Sounds of Frustration Intensify)\n",
    "\n",
    "Well, not right away, but Harry says to expect more headaches as the examples get less trivial from now on.  One common issue is that we may see different outputs than he documents in the book.  Some degree of divergence is common when the examples get more complex.\n",
    "\n",
    "## Django Project Structure $ \\ni $ \"apps\"\n",
    "\n",
    "(I hope I used that right; I meant to say something like \"entails\" or \"includes\"; in [Set notation that may be that a set \"owns\" the latter term](https://www.geeksforgeeks.org/set-notations-in-latex/).)\n",
    "\n",
    "The point is that Django expects us to organize semi-autonomous code into elements called \"apps\", and you can initialize a new app with\n",
    "\n",
    "`python manage.py startapp lists`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandError: 'lists' conflicts with the name of an existing Python module and cannot be used as an app name. Please try another name.\n"
     ]
    }
   ],
   "source": [
    "# trying it live; comment out after invoking:\n",
    "# !python manage.py startapp lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which constructs a new file structure and dir:\n",
    "\n",
    "~~~bash\n",
    ".\n",
    "├── 00_prereqs.ipynb\n",
    "├── 01_django_setup_functional_test.ipynb\n",
    "├── 02_unittest_module.ipynb\n",
    "├── 03_home_page.ipynb\n",
    "├── README.md\n",
    "├── db.sqlite3\n",
    "├── functional_tests.py\n",
    "├── geckodriver.log\n",
    "├── lists\n",
    "│   ├── __init__.py\n",
    "│   ├── admin.py\n",
    "│   ├── apps.py\n",
    "│   ├── migrations\n",
    "│   │   └── __init__.py\n",
    "│   ├── models.py\n",
    "│   ├── tests.py\n",
    "│   └── views.py\n",
    "├── manage.py\n",
    "├── superlists\n",
    "│   ├── __init__.py\n",
    "│   ├── __pycache__\n",
    "│   │   ├── __init__.cpython-36.pyc\n",
    "│   │   ├── settings.cpython-36.pyc\n",
    "│   │   ├── urls.cpython-36.pyc\n",
    "│   │   └── wsgi.cpython-36.pyc\n",
    "│   ├── settings.py\n",
    "│   ├── urls.py\n",
    "│   └── wsgi.py\n",
    "├── test_dev_book.yml\n",
    "└── workspace.code-workspace\n",
    "~~~\n",
    "\n",
    "Note the major handles and their implied logic: models, views, and tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Tests vs Functional Tests\n",
    "\n",
    "Then there's a procedural note here about how to think about the two types of tests mentioned so far.  Basically (as mentioned in the previous chapter), Functional tests are meant to recapitulate the gist of the user experience, testing your code \"from the outside\" and seeing what it gets when a big chunk of it is run.  Unit tests are written from the point of view of the developer, and generally (as I interpret it at this point) test smaller chunks of your code, \"from the inside\".\n",
    "\n",
    "He further specifies the general workflow you use in Test-Driven Development:\n",
    "\n",
    "1. You write a functional test meant to capture how you anticipate the user to interact with your app\n",
    "2. Once the functional test fails as expected, you think about what code you'd need to get it to pass.  But before you actually write that, you write unit tests to capture how you expect that code to work, and how those lines themselves could fail.\n",
    "3. When you have a failing unit test for what you expect to get you past (part of) the functional test, you write the smallest unit of *application code* that would do something useful.\n",
    "4. Then you iterate over steps 2 and 3 as needed until your functional test can pass.\n",
    "\n",
    "<div class=\"alert alert-info\">I'll just quote verbatim here the last text of this section which takes yet another stab at contextualizing the distinction between these two kinds of tests: \"<i>Functional tests should help you build an application with the right functionality, and guarantee you never accidentally break it.  Unit tests should help you write code that's clean and bug-free.</i>\"</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Testing in Django\n",
    "\n",
    "We need to modify that `lists/tests.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load lists/tests.py\n",
    "from django.test import TestCase\n",
    "\n",
    "# Create your tests here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok.  He notes how they prompt us by importing a class they expect to be useful, the `django.test.TestCase` class, which extends `unittest.TestCase`, adding functionality that may be relevant for creating web apps, specifically.\n",
    "\n",
    "Since this was added automatically by Django, rather than something we configured ourselves from the outset, Harry suggests adding a kind of crazy/silly test to it to ensure that it's being invoked and behaving as expected when the whole project is tested:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lists/tests.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lists/tests.py\n",
    "\n",
    "from django.test import TestCase\n",
    "\n",
    "class SmokeTest(TestCase):\n",
    "\n",
    "    def test_bad_math(self):\n",
    "        self.assertEqual(1 + 1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you invoke it (turns out this invocation *doesn't* require you fire up the server first in another window with `runserver`) by calling `python manage.py` in the main project, and specifying one additional keyword: `test`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test database for alias 'default'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System check identified no issues (0 silenced).\n",
      "Destroying test database for alias 'default'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FAIL: test_bad_math (lists.tests.SmokeTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Data\\projects\\test_driven_development\\lists\\tests.py\", line 7, in test_bad_math\n",
      "    self.assertEqual(1 + 1, 3)\n",
      "AssertionError: 2 != 3\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "%run manage.py test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Django's MVC, URLs, and View Functions\n",
    "\n",
    "Ok, then we cover that DJango has a *Model-View-Controller (MVC)* schema.  He says there are more details about how that's implemented in [their docs](https://docs.djangoproject.com/en/1.11/faq/general/).  The major points are that\n",
    "\n",
    "1. The server receives an HTTP request for a particular URL\n",
    "2. Django applies some logic to determine which *view function* should deal with that request, which is known as *resolving the URL*\n",
    "3. Then the view function returns the chosen HTTP response\n",
    "\n",
    "So we write a test to see how the program handles the root (\"`/`\") of our page, and put that in the `lists/tests.py` file (and don't mind that it overwrites that trivial/silly test previously committed to that file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lists/tests.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lists/tests.py\n",
    "\n",
    "from django.urls import resolve\n",
    "from django.test import TestCase\n",
    "from lists.views import home_page\n",
    "\n",
    "class HomePageTest(TestCase):\n",
    "\n",
    "    def test_root_url_resolves_to_home_page_view(self):\n",
    "        found = resolve('/')\n",
    "        self.assertEqual(found.func, home_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then he breaks down the code, saying that the `django.urls.resolve` function handles that second numbered point above: maps the URL to the *view function* that should be returned.  In this case, that function is `home_page`.  So then you're just checking that that is indeed what gets returned as the `func.` attribute of what gets returned by that `resolve` function when called (which we've assigned to the arbitrary variable `found`, in this case).\n",
    "\n",
    "The function that `resolve` returns in turn has the responsibility to yield the HTML content appropriate to the URL.  So, the third numbered item above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test database for alias 'default'..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E\n",
      "======================================================================\n",
      "ERROR: lists.tests (unittest.loader._FailedTest)\n",
      "----------------------------------------------------------------------\n",
      "ImportError: Failed to import test module: lists.tests\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\DCM0303\\Miniconda3\\envs\\test_dev_book\\lib\\unittest\\loader.py\", line 428, in _find_test_path\n",
      "    module = self._get_module_from_name(name)\n",
      "  File \"C:\\Users\\DCM0303\\Miniconda3\\envs\\test_dev_book\\lib\\unittest\\loader.py\", line 369, in _get_module_from_name\n",
      "    __import__(name)\n",
      "  File \"c:\\Data\\projects\\test_driven_development\\lists\\tests.py\", line 4, in <module>\n",
      "    from lists.views import home_page\n",
      "ImportError: cannot import name 'home_page'\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.000s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "System check identified no issues (0 silenced).\n",
      "Destroying test database for alias 'default'...\n"
     ]
    }
   ],
   "source": [
    "# %run lists/tests.py\n",
    "# %run manage.py test\n",
    "!python manage.py test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is to be expected, as there's no `home_page` file that we've written anywhere in this project yet.  This counts as an expected failure, so good practice, still.\n",
    "\n",
    "---\n",
    "\n",
    "As one procedural note about the complications of executing this on Windows: the expected result according to Percival is the `ImportError`, and I get that if I call `lists/tests.py` directly via the IPython `%run` magic, but if I instead invoke `manage.py` with that same magic, I instead see the aforementioned `AssertionError: 2 != 3`.  And the book specifically says to call it with \n",
    "\n",
    "`python manage.py test`\n",
    "\n",
    "from a terminal.  And if you instead use the shell magic to call it in that fashion from within the notebook, I *do* get the `ImportError`.  (Perhaps more trivially, I can confirm that if I invoke from within a terminal, it returns the expected `ImportError`, too; so it's only the `%run` magic executed on `manage.py` that confuses things.)  So behavior is a little inconsistent/unpredictable if you try to get clever about orchestrating everything from within a Notebook environment.  So watch out for that, going forward.\n",
    "\n",
    "---\n",
    "\n",
    "Moreover, Harry says that we have now written one failing functional test, and one failing unit test.  Now, I've resumed these notes after a couple weeks of absence, so I had to try to re-familiarize myself with which file constituted which kind of test.  As the contents of `lists/tests.py`'s `HomePageTest` inherits from `django.test.TestCase`, which in turn extends `unittest.TestCase`, I figured that should count as a Unit Test, but then even the sole class so far contained in the `functional_tests.py` file (\"`NewVistorTest`\") also inherits from `unittest.TestCase`, obviously the code base that a given chunk of test code extends is not a fool-proof indicator as to which it is (which maybe should be expected, as I don't think there's any `functionaltest` or similar module in Python).  Instead, I guess it's really all about the overall philosophy as covered above: if the logic within the file sounds like it's emulating how an end user may invoke your app, then it's a **functional** test; if it looks more wonk-ish or opaque from the outside, it's likely a **unit** test.\n",
    "\n",
    "Anyways, the point is that he's saying that you want to make sure you have an example of an expected failure with both a unit test *and* a functional test before you start writing code about what your application is actually supposed to *do*.  So we're ready to start that, now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But Harry says he's being a little facetiously or extremely plodding with the pacing here: we don't want to write the whole HTML code for the home page, nor even the entirety of the `home_page` function that's meant to return it.  Instead, we observe painfully incremental change to fix the existing unit test failure (the resulting code goes into the `lists/views.py` file, to accord with the overall architecture that Django expects):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lists/views.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lists/views.py\n",
    "\n",
    "from django.shortcuts import render\n",
    "\n",
    "# Create your views here.\n",
    "home_page = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test database for alias 'default'..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E\n",
      "======================================================================\n",
      "ERROR: test_root_url_resolves_to_home_page_view (lists.tests.HomePageTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Data\\projects\\test_driven_development\\lists\\tests.py\", line 9, in test_root_url_resolves_to_home_page_view\n",
      "    found = resolve('/')\n",
      "  File \"C:\\Users\\DCM0303\\Miniconda3\\envs\\test_dev_book\\lib\\site-packages\\django\\urls\\base.py\", line 27, in resolve\n",
      "    return get_resolver(urlconf).resolve(path)\n",
      "  File \"C:\\Users\\DCM0303\\Miniconda3\\envs\\test_dev_book\\lib\\site-packages\\django\\urls\\resolvers.py\", line 394, in resolve\n",
      "    raise Resolver404({'tried': tried, 'path': new_path})\n",
      "django.urls.exceptions.Resolver404: {'tried': [[<RegexURLResolver <RegexURLPattern list> (admin:admin) ^admin/>]], 'path': ''}\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.006s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "System check identified no issues (0 silenced).\n",
      "Destroying test database for alias 'default'...\n"
     ]
    }
   ],
   "source": [
    "!python manage.py test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that appears to match the anticipated result in the text.\n",
    "\n",
    "<div class=\"alert alert-info\">There's a box here about <b>Reading Tracebacks</b>; he says to start at the bottom with the reported error itself (\"<code>django.urls.exceptionsResolver404</code>\").  In our case that isn't immediately informative, so you check next for one of the files that <i>you wrote</i> in the traceback, from the top down; specifically, which of your <b>tests</b> failed.  In this case, we expect that the most recent one that we wrote should be at fault: the <code>test_root_url_resolves_to_home_page_view</code> in the <code>lists/tests.py</code> file, which it is.  Then, you look for more context within that test: the call to <code>resolve('/')</code>.  But then the trail kind of goes cold, unless you know enough about how Django works to interpret the downstream stuff.  But it's basically saying in this case that Django can't find the content meant to map to the <code>'/'</code> URL.</div>\n",
    "\n",
    "## `urls.py`\n",
    "\n",
    "Harry says that the solution is to write a `superlists/urls.py` file to provide the mappings of URLs to HTML files.  This should've been written by the original \n",
    "\n",
    "`python manage.py startapp lists`\n",
    "\n",
    "call to kick off this notebook; let's check that file's existence and content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load superlists/urls.py\n",
    "\"\"\"superlists URL Configuration\n",
    "\n",
    "The `urlpatterns` list routes URLs to views. For more information please see:\n",
    "    https://docs.djangoproject.com/en/1.11/topics/http/urls/\n",
    "Examples:\n",
    "Function views\n",
    "    1. Add an import:  from my_app import views\n",
    "    2. Add a URL to urlpatterns:  url(r'^$', views.home, name='home')\n",
    "Class-based views\n",
    "    1. Add an import:  from other_app.views import Home\n",
    "    2. Add a URL to urlpatterns:  url(r'^$', Home.as_view(), name='home')\n",
    "Including another URLconf\n",
    "    1. Import the include() function: from django.conf.urls import url, include\n",
    "    2. Add a URL to urlpatterns:  url(r'^blog/', include('blog.urls'))\n",
    "\"\"\"\n",
    "from django.conf.urls import url\n",
    "from django.contrib import admin\n",
    "\n",
    "urlpatterns = [\n",
    "    url(r'^admin/', admin.site.urls),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then he mentions the general structure of what that `django.conf.urls.url` function expects:\n",
    "\n",
    "1. A Regex string defining to which kinds of URLs it should apply\n",
    "2. Where it should send the request: \"*either to a view function you've imported, or maybe to another `urls.py` file somewhere else*\"\n",
    "\n",
    "Harry suggests following a couple of the recommendations from the default-formattted file's doc strings.  Specifically, try plopping in a regex for an empty line (\"`^$`\") and replacing that reference to the `django.contrib.admin.site.urls` func, which he says we won't use.  Instead, substitute the `home_page` func from our `views` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting superlists/urls.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile superlists/urls.py\n",
    "from django.conf.urls import url\n",
    "from lists import views\n",
    "\n",
    "urlpatterns = [\n",
    "    url(r'^$', views.home_page, name='home'),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, re-try the most recent test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test database for alias 'default'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"manage.py\", line 22, in <module>\n",
      "    execute_from_command_line(sys.argv)\n",
      "  File \"C:\\Users\\DCM0303\\Miniconda3\\envs\\test_dev_book\\lib\\site-packages\\django\\core\\management\\__init__.py\", line 364, in execute_from_command_line\n",
      "    utility.execute()\n",
      "  File \"C:\\Users\\DCM0303\\Miniconda3\\envs\\test_dev_book\\lib\\site-packages\\django\\core\\management\\__init__.py\", line 356, in execute\n",
      "    self.fetch_command(subcommand).run_from_argv(self.argv)\n",
      "  File \"C:\\Users\\DCM0303\\Miniconda3\\envs\\test_dev_book\\lib\\site-packages\\django\\core\\management\\commands\\test.py\", line 29, in run_from_argv\n",
      "    super(Command, self).run_from_argv(argv)\n",
      "  File \"C:\\Users\\DCM0303\\Miniconda3\\envs\\test_dev_book\\lib\\site-packages\\django\\core\\management\\base.py\", line 283, in run_from_argv\n",
      "    self.execute(*args, **cmd_options)\n",
      "  File \"C:\\Users\\DCM0303\\Miniconda3\\envs\\test_dev_book\\lib\\site-packages\\django\\core\\management\\base.py\", line 330, in execute\n",
      "    output = self.handle(*args, **options)\n",
      "  File \"C:\\Users\\DCM0303\\Miniconda3\\envs\\test_dev_book\\lib\\site-packages\\django\\core\\management\\commands\\test.py\", line 62, in handle\n",
      "    failures = test_runner.run_tests(test_labels)\n",
      "  File \"C:\\Users\\DCM0303\\Miniconda3\\envs\\test_dev_book\\lib\\site-packages\\django\\test\\runner.py\", line 602, in run_tests\n",
      "    self.run_checks()\n",
      "  File \"C:\\Users\\DCM0303\\Miniconda3\\envs\\test_dev_book\\lib\\site-packages\\django\\test\\runner.py\", line 562, in run_checks\n",
      "    call_command('check', verbosity=self.verbosity)\n",
      "  File \"C:\\Users\\DCM0303\\Miniconda3\\envs\\test_dev_book\\lib\\site-packages\\django\\core\\management\\__init__.py\", line 131, in call_command\n",
      "    return command.execute(*args, **defaults)\n",
      "  File \"C:\\Users\\DCM0303\\Miniconda3\\envs\\test_dev_book\\lib\\site-packages\\django\\core\\management\\base.py\", line 330, in execute\n",
      "    output = self.handle(*args, **options)\n",
      "  File \"C:\\Users\\DCM0303\\Miniconda3\\envs\\test_dev_book\\lib\\site-packages\\django\\core\\management\\commands\\check.py\", line 68, in handle\n",
      "    fail_level=getattr(checks, options['fail_level']),\n",
      "  File \"C:\\Users\\DCM0303\\Miniconda3\\envs\\test_dev_book\\lib\\site-packages\\django\\core\\management\\base.py\", line 359, in check\n",
      "    include_deployment_checks=include_deployment_checks,\n",
      "  File \"C:\\Users\\DCM0303\\Miniconda3\\envs\\test_dev_book\\lib\\site-packages\\django\\core\\management\\base.py\", line 346, in _run_checks\n",
      "    return checks.run_checks(**kwargs)\n",
      "  File \"C:\\Users\\DCM0303\\Miniconda3\\envs\\test_dev_book\\lib\\site-packages\\django\\core\\checks\\registry.py\", line 81, in run_checks\n",
      "    new_errors = check(app_configs=app_configs)\n",
      "  File \"C:\\Users\\DCM0303\\Miniconda3\\envs\\test_dev_book\\lib\\site-packages\\django\\core\\checks\\urls.py\", line 16, in check_url_config\n",
      "    return check_resolver(resolver)\n",
      "  File \"C:\\Users\\DCM0303\\Miniconda3\\envs\\test_dev_book\\lib\\site-packages\\django\\core\\checks\\urls.py\", line 26, in check_resolver\n",
      "    return check_method()\n",
      "  File \"C:\\Users\\DCM0303\\Miniconda3\\envs\\test_dev_book\\lib\\site-packages\\django\\urls\\resolvers.py\", line 256, in check\n",
      "    for pattern in self.url_patterns:\n",
      "  File \"C:\\Users\\DCM0303\\Miniconda3\\envs\\test_dev_book\\lib\\site-packages\\django\\utils\\functional.py\", line 35, in __get__\n",
      "    res = instance.__dict__[self.name] = self.func(instance)\n",
      "  File \"C:\\Users\\DCM0303\\Miniconda3\\envs\\test_dev_book\\lib\\site-packages\\django\\urls\\resolvers.py\", line 407, in url_patterns\n",
      "    patterns = getattr(self.urlconf_module, \"urlpatterns\", self.urlconf_module)\n",
      "  File \"C:\\Users\\DCM0303\\Miniconda3\\envs\\test_dev_book\\lib\\site-packages\\django\\utils\\functional.py\", line 35, in __get__\n",
      "    res = instance.__dict__[self.name] = self.func(instance)\n",
      "  File \"C:\\Users\\DCM0303\\Miniconda3\\envs\\test_dev_book\\lib\\site-packages\\django\\urls\\resolvers.py\", line 400, in urlconf_module\n",
      "    return import_module(self.urlconf_name)\n",
      "  File \"C:\\Users\\DCM0303\\Miniconda3\\envs\\test_dev_book\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"c:\\Data\\projects\\test_driven_development\\superlists\\urls.py\", line 5, in <module>\n",
      "    url(r'^$', views.home_page, name='home'),\n",
      "  File \"C:\\Users\\DCM0303\\Miniconda3\\envs\\test_dev_book\\lib\\site-packages\\django\\conf\\urls\\__init__.py\", line 85, in url\n",
      "    raise TypeError('view must be a callable or a list/tuple in the case of include().')\n",
      "TypeError: view must be a callable or a list/tuple in the case of include().\n"
     ]
    }
   ],
   "source": [
    "!python manage.py test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that's a whole crap-ton of calls in the traceback.  But anyways, the final line matches what's anticipated from the text.  Harry says it counts as progress, insofar as we're no longer seeing a `404`-type error about your app not finding where to look.\n",
    "\n",
    "He says that the nature of the error above (that the thing ultimately returned in response to the request is not callable) indicates that the intended connection between the home URL (\"`/`\") and the `home_page = None` line in `lists/views.py` *is* being made.  So all of that was another very small, cautious step to ensure that your test is working as anticipated, and now justifies replacing `None` in that line with something more interesting.\n",
    "\n",
    "So the next change is to replace that line in `lists/views.py` with something callable (first, a cell to refresh our knowledge of the preexisting contents of that file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load lists/views.py\n",
    "\n",
    "from django.shortcuts import render\n",
    "\n",
    "# Create your views here.\n",
    "home_page = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lists/views.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lists/views.py\n",
    "\n",
    "from django.shortcuts import render\n",
    "\n",
    "# Create your views here.\n",
    "def home_page():\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then repeat the test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test database for alias 'default'...\n",
      "System check identified no issues (0 silenced).\n",
      "Destroying test database for alias 'default'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python manage.py test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which constitutes the first passing unit test in the project/app/book.  He suggests it might be a good time to commit to your repo.\n",
    "\n",
    "Ok, then we edit the `lists/tests.py` file, and add a second test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load lists/tests.py\n",
    "\n",
    "from django.urls import resolve\n",
    "from django.test import TestCase\n",
    "from lists.views import home_page\n",
    "\n",
    "class HomePageTest(TestCase):\n",
    "\n",
    "    def test_root_url_resolves_to_home_page_view(self):\n",
    "        found = resolve('/')\n",
    "        self.assertEqual(found.func, home_page)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lists/tests.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lists/tests.py\n",
    "from django.urls import resolve\n",
    "from django.test import TestCase\n",
    "from django.http import HttpRequest\n",
    "\n",
    "from lists.views import home_page\n",
    "\n",
    "class HomePageTest(TestCase):\n",
    "\n",
    "    def test_root_url_resolves_to_home_page_view(self):\n",
    "        found = resolve('/')\n",
    "        self.assertEqual(found.func, home_page)\n",
    "\n",
    "    def test_home_page_returns_correct_html(self):\n",
    "        request = HttpRequest()\n",
    "        response = home_page(request)\n",
    "        html = response.content.decode('utf8')\n",
    "        self.assertTrue(html.startswith('<html>'))\n",
    "        self.assertIn('<title>To-Do lists</title>', html)\n",
    "        self.assertTrue(html.endswith('</html>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we cover what the above is supposed to do:\n",
    "\n",
    "1. (line 15) The `django.http.HttpRequest` object stores what Django will receive to process when a user issues a request for a URL within your site\n",
    "2. (line 16) You pass the above `HttpRequest` object to your `home_page` view/function\n",
    "3. (line 17) You grab the HTML content of the page returned by the above view, by decoding from bytes\n",
    "4. (lines 18 & 20) You check that the whole decoded page is enclosed within an `<html></html>` tag, to constitute legal HTML\n",
    "5. (line 19) You check that the title is as expected\n",
    "\n",
    "At this point, though, the `home_page` view/function written to `lists/views` actually takes no args, so this is an expected failure case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E.\n",
      "======================================================================\n",
      "ERROR: test_home_page_returns_correct_html (lists.tests.HomePageTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Data\\projects\\test_driven_development\\lists\\tests.py\", line 15, in test_home_page_returns_correct_html"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test database for alias 'default'...\n",
      "System check identified no issues (0 silenced).\n",
      "Destroying test database for alias 'default'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "    response = home_page(request)\n",
      "TypeError: home_page() takes 0 positional arguments but 1 was given\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.001s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    }
   ],
   "source": [
    "!python manage.py test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Unit-Test/Code cycle\n",
    "\n",
    "At this point, Harry says that you can settle into a rhythm: you write a new unit test to check for the successful functioning of the next significant line/chunk of code that you intend to write in your application, check that it fails, then write the application, and check that it succeeds.\n",
    "\n",
    "What's left unsaid (as I interpret it) is that it's still a matter of habit and art to know *what* to test, exactly.  For instance, I can see why it might be important to check that your returned home page in the above test is enclosed within the `html` tags, but why bother to check that the title is as expected, as opposed to other stuff?  I'm not much of a web developer (well, I'm not *any* of a web developer), so which characteristics are important to check and maintain don't really jump out to me.  And so (again, I interpret) it goes for other applications and code bases: you'd have to be decent at building various types of stuff to know what was important vs trivial to ensure was working in the code.  But so much for my aside...\n",
    "\n",
    "He does, however, offer that when you start out, your tests will need to be more painfully, laboriously minimal and thorough, with the insinuation that once you get a lot better at a subject (like web development), you'll build up a better working knowledge about what is likely to fail in practice, and maybe focus your written tests on those pain points.  Of course, that's kind of me reading between the lines; he can't really encourage deviation from the *best practice* of writing tests for every little damn thing, because you can't always write off a given failure scenario as irrelevant.\n",
    "\n",
    "But anyways, at this point we review the above steps, in order to try to build up a habit of knowing what constitutes a good minimal test in this case, based on the application code you expect to write:\n",
    "\n",
    "* Desired functionality:\n",
    "    * Your `lists/views.py` needs to include a function to return your home page (but not necessarily actually pass anything *real*, at this point):\n",
    "        ~~~python\n",
    "        def home_page(request):\n",
    "            pass\n",
    "        ~~~\n",
    "* The corresponding test to write:\n",
    "    * Check the content of what gets passed, by decoding to standard HTML:\n",
    "        ~~~python\n",
    "        html = response.content.decode('utf8')\n",
    "        ~~~\n",
    "* Anticipated outcome:\n",
    "    * You told your function to just `pass` (minimal incremental functionality is added to application code), so it should raise an error saying `NoneType` lacks the attribute:\n",
    "        ~~~bash\n",
    "        AttributeError: 'NoneType' object has no attribute 'content'\n",
    "        ~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Desired functionality:\n",
    "    * Once you've ensured the first test fails, augment your `home_page` view to instead at least return an empty `django.http.HttpResponse` object:\n",
    "        ~~~python\n",
    "        def home_page(request):\n",
    "            return HttpResponse\n",
    "        ~~~\n",
    "* Corresponding test:\n",
    "    * Check that it's formatted as HTML:\n",
    "        ~~~python\n",
    "        self.assertTrue(html.startswith('<html>'))\n",
    "        ~~~\n",
    "* Anticipated outcome:\n",
    "    * `HttpResponse` objects don't have that tag unless you pass them some kind of content to structure as a valid HTML page, so:\n",
    "        ~~~python\n",
    "        AssertionError: False is not True\n",
    "        ~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Desired functionality:\n",
    "    * When *that* test fails, move on to giving the `HttpResponse` object some properly-formatted HTML text to sink its teeth into:\n",
    "        ~~~python\n",
    "        def home_page(request):\n",
    "            content = '<html><title>To-Do lists</title></html>'\n",
    "            return HttpResponse(content)\n",
    "\n",
    "So now, instead of mapping out hypotheticals in markdown, actually augment the specified files.  The `tests.py` file is up to date, so just update `views.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lists/views.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lists/views.py\n",
    "\n",
    "from django.shortcuts import render\n",
    "from django.http import HttpResponse\n",
    "\n",
    "def home_page(request):\n",
    "    content = '<html><title>To-Do lists</title></html>'\n",
    "    return HttpResponse(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test database for alias 'default'...\n",
      "System check identified no issues (0 silenced).\n",
      "Destroying test database for alias 'default'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.001s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python manage.py test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's our unit tests up-to-date and passing; now he says to check the functional tests:\n",
    "\n",
    "<div class=\"alert alert-warning\"><p>He notes that you have to \"spin up the dev server again\"; I've gone long enough between edits to this notebook to forget how to do that.  Turns out it's</p>\n",
    "<code>python manage.py runserver</code>\n",
    "<br>\n",
    "<p>in a separate terminal.  When I fail to do that I get different errors than in the book.</p></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F\n",
      "======================================================================\n",
      "FAIL: test_can_start_a_list_and_retrieve_it_later (__main__.NewVisitorTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"functional_tests.py\", line 20, in test_can_start_a_list_and_retrieve_it_later\n",
      "AssertionError: Finish the test!\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 6.568s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    }
   ],
   "source": [
    "# %run functional_tests.py\n",
    "!python functional_tests.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is where we remember that we wrote the original functional test to be just a placeholder.  Anyways, that's enough for one chapter; Harry recommends at this point that we commit to the repo, and check out what we've done recently, with:\n",
    "\n",
    "~~~bash\n",
    "git diff  # should show our new test in tests.py, and the view in views.py\n",
    "git commit -am \"Basic view now returns minimal HTML\"\n",
    "git log --oneline\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "132b65336a829e6530b03c2ac43b744d0aab8a1a430c616a24f2cd56f79cc52e"
  },
  "kernelspec": {
   "display_name": "Python 3.6.15 64-bit ('test_dev_book': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
